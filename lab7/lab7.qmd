---
title: "lab7: machine learning I"
author: "Elsa Chen (A16632961)"
format: pdf
toc: TRUE
---

Today we're exploring first part of machine learing, clustering - finding patterns in data and dimensionality reduction

## Clustering
### Starting with "k-means" clustering (`kmeans()`)

```{r}
# making up data
hist(rnorm(1000))
```

```{r}
temp <- c(rnorm(30, -3), rnorm(30, 3))
x <- cbind(x = temp, y = rev(temp))
plot(x)
```

now running `kmeans()`

```{r}
km <- kmeans(x, centers = 2)
km
attributes(km)
```
> Q. How many points in each cluster?

```{r}
km$size
```

> Q. What component of your result object details cluster assignment/membership?

```{r}
km$cluster
```

> Q. What are centers/mean values of each cluster?

```{r}
km$centers
```

> Q. Make a plot of your data showing your clustering results (groupings/clusters and cluster centers)

```{r}
plot(x, col = km$cluster)
points(km$centers, col = "blue", pch = 15)
```

> Q. Run `kmeans()` again and cluster in 4 groups and plot results

```{r}
km2 <- kmeans(x, centers = 4)
plot(x, col = km2$cluster)
```
### Hierarchical Clustering

reveal structure in data by grouping points into a smaller number of clusters
`hclust()` this function does not take our imput data directly but wants a "distance matrix" that details how (dis)similar all our input points are to each other. 

```{r}
hc <- hclust(dist(x)) # dist() measures distance pairwise between each point
plot(hc)
abline(h = 10, col = "red")
```

to get my main result, main cluster

```{r}
grps <- cutree(hc, h = 10)
```
```{r}
plot(x, col = grps)
```

## Principal Component Analysis

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(x)
```
```{r}
head(x)
```
> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

```{r}
x <- read.csv(url, row.names = 1)
```

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```
> Q3: Changing what optional argument in the above barplot() function results in the following plot?

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

> Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(10), pch=16)
```
if the points lie on the diagonal, there is an association between the country on the x and y axes

hard to understand even for small data set, so let's use PCA `prcomp()`

```{r}
pca <- prcomp(t(x))
summary(pca)
pca$x
```

```{r}
colors <- c("orange", "red", "blue", "green")
plot(pca$x[,1], pca$x[,2], col = colors, pch = 16, 
     xlab = "PC1", ylab = "PC2")
```
the "rotation" component tells us how much the original variables contribute to the new PCs

```{r}
barplot( pca$rotation[,1], las=2 )
```
PCA is useful for gaining insight into high dimensional data that is difficult to examine in other ways

### PCA of RNAseq data 

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

```{r}
## Again we have to take the transpose of our data 
pca <- prcomp(t(rna.data), scale=TRUE) # scales all data
summary(pca)
```
> Q: how many genes in this dataset?

```{r}
nrow(rna.data)
```

```{r}
## Simple un polished plot of pc1 and pc2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2")
```
```{r}
# prettier ggplot
library(tidyverse)
res <- as.data.frame(pca$x)
head(res)
ggplot(res) +
  aes(x = PC1, y = PC2) +
  geom_point()  # PC1 spread is more important because in summary(pca), PC1 accounts for 92% of the variance
  
```

```{r}
kmeans(pca$x[,1], centers = 2)
```

